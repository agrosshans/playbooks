---

#######################################
####### Cluster Managenent Hosts ######
#######################################

- hosts: ha-web-mgmt
  become: yes
  remote_user: root

  vars_files:
    - vars/main.yml

  tasks:

  # (Management Node) Set SE Linux to be permissive for now (will be restrictive later on)
  - name: (Management Node) Set SE Linux to be permissive for now (will be restrictive later on)
    lineinfile:
      path: /etc/selinux/config
      regexp: '^SELINUX='
      line: 'SELINUX=permissive'

    tags: security

  # (Management Node) Install necessary Packages
  - name: (Management Node) Install necessary Packages
    yum: name={{item}} state=installed
    with_items:
      - policycoreutils-python
      - rsync
      - lsof
      - bind-utils
      - yum-utils
      - iscsi-initiator-utils
      - device-mapper-multipath
      - "@High Availability Management"

    tags: packages

  # (Management Node) add local repo for CentOS updates
  - name: (Management Node) add local repo for CentOS updates
    yum_repository:
      name: CentOS-Updates-local
      description: CentOS-6 - Updates from local repo
      baseurl: https://192.168.1.199/shares/CENTOS%2520UPDATES/6/updates/x86_64/
      gpgkey: file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6
      repo_gpgcheck: no
      sslverify: no

    tags: updates

  # Run (Management Node) Run yum-complete-transaction to clear out any issue before next step
  - name: (Management Node) Run yum-complete-transaction to clear out any issue before next step
    command: /usr/sbin/yum-complete-transaction
    tags: packages


    # (Management Node) Update each node to take in the latest patches and security updates
  - name: (Management Node) Update each node to take in the latest patches and security updates
    yum:
      name: '*'
      enablerepo: CentOS-Updates-local
      state: latest

    tags: updates

  # (Management Node) Restart host after SE Linux config change and latest patches and security updates
  - name: (Management Node) Restart host after SE Linux config change and latest patches and security updates
    command: shutdown -r now "Activating ramdisk LVM changes"

    tags: reboot

  # (Management Node) Waiting for  host '{{ ansible_host }}' to become available
  - name: (Management Node) waiting 10 mn for '{{ ansible_host }}' on port '{{ ansible_port }}' to come back
    wait_for:
      port: '{{ ansible_port }}'
      host: '{{ ansible_host }}'
      search_regex: OpenSSH
      delay: 300
      timeout: 2400 # can take longer after FS are automatically relabeled when changing SELinux from disabled mode to permissive or enforcing mode.
    become: no
  
    vars:
      ansible_connection: local
    tags: reboot

  # (Management Node) Update /etc/hosts
  - name: (Management Node) Update /etc/hosts
    copy:
      src: hosts
      dest: /etc/hosts
      owner: root
      group: root
      mode: 0644
    tags: network

  # (Management Node) Create ifcfg-eth1
  - name: (Management Node) Create ifcfg-eth1
    template:
      src: ifcfg-eth1.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth1
    when: (ansible_bond0 is undefined) # run: ansible ha-web1 -i inventory -m setup -a 'filter(xxx)' to see if it's define
    ignore_errors: yes
    tags: network      

  # (Management Node) Create ifcfg-eth2
  - name: (Management Node) Create ifcfg-eth2
    template:
      src: ifcfg-eth2.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth2
    tags: network      

  # (Management Node) Create ifcfg-eth3
  - name: (Management Node) Create ifcfg-eth3
    template:
      src: ifcfg-eth3.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth3
    ignore_errors: yes
    tags: network      

   # (Management Node) Create ifcfg-eth4
  - name: (Management Node) Create ifcfg-eth4
    template:
      src: ifcfg-eth4.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth4
    tags: network      

  # (Management Node) Restart the network services to activate the changes
  - name: (Management Node) Restart the network services to activate the changes
    service:
      name: network
      state: restarted
      args: "{{ item }}"
    with_items:
      - eth1
      - eth2
      - eth3
      - eth4
    tags: network      

  # (Management Node) Create bond configuration files for the public and cluster interconnect networks
  - name: (Management Node) Create bond configuration files for the public and cluster interconnect networks
    blockinfile:
         path: /etc/modprobe.d/bonding.conf 
         create: yes
         block: |
          alias bond0 bonding
          alias bond1 bonding
    tags: network      

  # (Management Node) Create ifcfg-bond0
  - name: (Management Node) Create ifcfg-bond0
    template:
      src: ifcfg-bond0.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-bond0
    when: (ansible_bond0 is not defined)
    ignore_errors: yes
    tags: network      

  # (Management Node) Create ifcfg-bond1
  - name: (Management Node) Create ifcfg-bond1
    template:
      src: ifcfg-bond1.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-bond1
    when: (ansible_bond1 is not defined)
    ignore_errors: yes
    tags: network      

  # (Management Node) Modify the interface file for the first public interface
  - name: (Management Node) Modify the interface file for the first public interface
    template:
      src: ifcfg-eth1-bond.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth1
    tags: network      

  # (Management Node) Modify the interface file for the cluster interconnect
  - name: (Management Node) Modify the interface file for the cluster interconnect
    template:
      src: ifcfg-eth3-bond.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth3
    tags: network      

  # (Management Node) Restart the network services to activate the changes
  - name: (Management Node) Restart the network services to activate the changes
    service:
      name: network
      state: restarted
      args: "{{ item }}"
    with_items:
      - eth1
      - eth2
      - eth3
      - eth4
    tags: network

  # (Management Node) Configure Firewall for webservice clusters
  - name: (Management Node) Configure Firewall for webservice clusters
    template:
      src: haweb-iptables.j2
      dest: /etc/sysconfig/iptables
    tags: network      

  - name: (First Node) Reload Firewall for webservice clusters
    service:
      name: iptables
      enabled: yes
      state: restarted
    tags: network

#######################################
####### First Cluster Node Hosts ######
#######################################

- hosts: ha-web1
  become: yes
  remote_user: root
  tasks:

  # (First Node) Set SE Linux to be persistent across reboots
  - name: (First Node) Set SE Linux to be permissive for now (will be restrictive later on)
    lineinfile:
      path: /etc/selinux/config
      regexp: '^SELINUX='
      line: 'SELINUX=permissive'

    tags: security

  # (First Node) Install necessary Packages
  - name: (First Node) Install necessary Packages
    yum: name={{item}} state=installed
    with_items:
      - policycoreutils-python
      - rsync
      - lsof
      - bind-utils
      - yum-utils
      - iscsi-initiator-utils
      - device-mapper-multipath
      - "@High Availability"

    tags: packages


  # (First Node) add local repo for CentOS updates
  - name: (First Node) add local repo for CentOS updates
    yum_repository:
      name: CentOS-Updates-local
      description: CentOS-6 - Updates from local repo
      baseurl: https://192.168.1.199/shares/CENTOS%2520UPDATES/6/updates/x86_64/
      gpgkey: file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6
      repo_gpgcheck: no
      sslverify: no

    tags: updates

  # (First Node) Run yum-complete-transaction to clear out any issue before next step
  - name: (First Node) Run yum-complete-transaction to clear out any issue before next step
    command: /usr/sbin/yum-complete-transaction
    tags: packages


  # (First Node) Update the node to take in the latest patches and security updates
  - name: (First Node) Update each node to take in the latest patches and security updates
    yum:
      name: '*'
      enablerepo: CentOS-Updates-local
      state: latest

    tags: updates

  # (First Node) Restart host after SE Linux config change and latest patches and security updates
  - name: (First Node) Restart host after SE Linux config change
    command: shutdown -r now "Activating ramdisk LVM changes"

    tags: reboot

  # (First Node) Waiting for  host '{{ ansible_host }}' to become available
  - name: (First Node) waiting 10 mn for '{{ ansible_host }}' on port '{{ ansible_port }}' to come back
    wait_for:
      port: '{{ ansible_port }}'
      host: '{{ ansible_host }}'
      search_regex: OpenSSH
      delay: 300
      timeout: 2400 # can take longer after FS are automatically relabeled when changing SELinux from disabled mode to permissive or enforcing mode.
    become: no
  
    vars:
      ansible_connection: local
    tags: reboot

  # (First Node) Copy files/hosts to remote hosts
  - name: (First Node) Copy files/hosts to remote hosts
    copy:
      src: hosts
      dest: /etc/hosts
      owner: root
      group: root
      mode: 0644
    tags: network

  # (First Node) Create ifcfg-eth1
  - name: (First Node) Create ifcfg-eth1
    template:
      src: ifcfg-eth1.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth1
    when: (ansible_bond0 is undefined) # run: ansible ha-web1 -i inventory -m setup -a 'filter(xxx)' to see if it's define
    ignore_errors: yes
    tags: network      

  # (First Node) Create ifcfg-eth2
  - name: (First Node) Create ifcfg-eth2
    template:
      src: ifcfg-eth2.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth2
    tags: network      

  # (First Node) Create ifcfg-eth3
  - name: (First Node) Create ifcfg-eth3
    template:
      src: ifcfg-eth3.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth3
    ignore_errors: yes
    tags: network      

   # (First Node) Create ifcfg-eth4
  - name: (First Node) Create ifcfg-eth4
    template:
      src: ifcfg-eth4.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth4
    tags: network      

  # (First Node) Restart the network services to activate the changes
  - name: (First Node) Restart the network services to activate the changes
    service:
      name: network
      state: restarted
      args: "{{ item }}"
    with_items:
      - eth1
      - eth2
      - eth3
      - eth4
    tags: network

  # (First Node) Create bond configuration files for the public and cluster interconnect networks
  - name: (First Node) Create bond configuration files for the public and cluster interconnect networks
    blockinfile:
         path: /etc/modprobe.d/bonding.conf 
         create: yes
         block: |
          alias bond0 bonding
          alias bond1 bonding
    tags: network

  # (First Node) Create ifcfg-bond0
  - name: (First Node) Create ifcfg-bond0
    template:
      src: ifcfg-bond0.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-bond0
    when: (ansible_bond0 is not defined)
    ignore_errors: yes
    tags: network      

  # (First Node) Create ifcfg-bond1
  - name: (First Node) Create ifcfg-bond1
    template:
      src: ifcfg-bond1.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-bond1
    when: (ansible_bond1 is not defined)
    ignore_errors: yes
    tags: network      

  # (First Node)  Modify the previous setup of ifcfg-ethX to refers the bonding for the public interface
  - name: (First Node)  Modify the previous setup of ifcfg-ethX to refers the bonding for the public interface
    template:
      src: ifcfg-eth1-bond.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth1
    tags: network

  - name: (First Node) Modify the previous setup of ifcfg-ethX to refers to bonding for the cluster interconnect
    template:
      src: ifcfg-eth3-bond.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth3
    tags: network

  # (First Node) restart network service once ifcfg-ethX completion
  - name: (First Node) restart network service once ifcfg-ethX completion
    service:
      name: network
      state: restarted
      args: "{{ item }}"
    with_items:
      - eth1
      - eth2
      - eth3
      - eth4
    tags: network  

  # (First Node) copy files/hosts to remote hosts
  - name: (First Node) copy files/hosts to remote hosts
    copy:
      src: hosts
      dest: /etc/hosts
      owner: root
      group: root
      mode: 0644
    tags: network      

  # (First Node) Configure Firewall for webservice clusters
  - name: (First Node) Configure Firewall for webservice clusters
    template:
      src: haweb-iptables.j2
      dest: /etc/sysconfig/iptables
    tags: network      

  # (First Node) Reload Firewall for webservice clusters
  - name: (First Node) Reload Firewall for webservice clusters
    service:
      name: iptables
      enabled: yes
      state: restarted
    tags: network      
  
  # (First Node) start iSCSI service and enable it
  - name: (First Node) start iSCSI service and enable it
    service:
      name: iscsi
      state: started
      enabled: yes
    tags: storage

  # (First Node) Install Multipathing
  - name: (First Node) Install Multipathing
    yum:
      name: device-mapper-multipath.x86_64
      state: latest
    tags: storage

  # (First Node) Start Multipathing daemon
  - name: (First Node) Start Multipathing daemon
    service:
      name: multipathd
      state: started
      enabled: yes
    tags: storage

  # (First Node) iSCSI Discovering Targets
  - name: (First Node) iSCSI Discovering Targets
    command: iscsiadm -m discovery -t st -p 192.168.1.199:3260
    tags: storage

  # (First Node) iSCSI log into an individual target
  - name: (First Node) iSCSI log into an individual target
    command: iscsiadm -m node -l -T iqn.1992-04.com.emc:storage.ix2-200-THUBCE.iscsi0 -p 192.168.1.199:3260
    tags: storage

  # (First Node) Configure Multipathing
  - name: (First Node) Configure Multipathing 
    copy:
      remote_src: yes
      src: /usr/share/doc/device-mapper-multipath-0.4.9/multipath.conf
      dest: /etc/multipath.conf
      owner: root
      group: root
      mode: 0644
    tags: storage

  # (First Node) Turn user_friendly_names for multipath to no
  - name: (First Node) Turn user_friendly_names for multipath to no
    lineinfile:
      path: /etc/multipath.conf
      regexp: '	user_friendly_names yes'
      line: '	user_friendly_names no'
    tags: storage

  # (First Node) Restart Multipathing daemon
  - name: (First Node) Restart Multipathing daemon
    service:
      name: multipathd
      state: restarted
      enabled: yes
    tags: storage


  # (First Node) add a device wwid to the wwids file (/etc/multipath/wwids)
  - name: (First Node) add a device wwid to the wwids file (/etc/multipath/wwids)
    shell: multipath -a /dev/dm-2
    tags: storage

  # (First Node) Wait 10 seconds to continue
  - name: (First Node) Wait 10 seconds to continue
    wait_for:
      timeout: 10
      sleep: 5
    tags: storage

  # (First Node) get the wwid from the wwids file (/etc/multipath/wwids)
  - name: (First Node) get the wwid from the wwids file (/etc/multipath/wwids)
    shell: /bin/cat /etc/multipath/wwids  | tail -1 | sed -e 's/^\///' | sed -e 's/\/$//'
    register: wwid
    tags: storage

    ##  - debug: msg="{{ wwid.stdout }}"

  # (First Node) copy multipath.conf template
  - name: (First Node) copy multipath.conf template
    vars:
      InitiatorName: "{{ wwid.stdout }}"
    template:
      src: multipath.conf.j2
      dest: /etc/multipath.conf
    tags: storage

  # (First Node) Fetch /etc/multipath.conf from First Node to Others
  - name: (First Node) Fetch /etc/multipath.conf from First Node to Others
    fetch:
      src: /etc/multipath.conf
      dest: files/
      flat: yes
    tags: storage

  # (First Node) Restart Multipathing daemon
  - name: (First Node) Restart Multipathing daemon again
    service:
      name: multipathd
      state: restarted
      enabled: yes
    tags: storage

  # (First Node) Ensure PV does exist
  - name: (First Node) Ensure PV does exist
    stat:
      path: /dev/mapper/ha-webp1
    register: pv
    tags: storage

  - debug:
      msg: "/dev/mapper/ha-webp1 PV does exist and is a link"
    when: pv.stat.islnk is defined
    tags: storage

  # (First Node) pvcreate /dev/mapper/ha-webp1
  - name: (First Node) pvcreate /dev/mapper/ha-webp1
    lvg:
      vg: HA-Web-VG
      pvs: /dev/mapper/ha-webp1
      vg_options: "-c n"
    when: pv.stat.islnk is not defined
    tags: storage

  # (First Node) lvcreate ha-web-lvol1
  - name: (First Node) lvcreate ha-web-lvol1
    lvol:
      vg: HA-Web-VG
      lv: ha-web-lvol1
      size: 10G
    when: pv.stat.islnk is defined
    tags: storage

  # (First Node) Create a ext4 filesystem on /dev/mapper/HA--Web--VG-ha--web--lvol1
  - name: (First Node) Create a ext4 filesystem on /dev/mapper/HA--Web--VG-ha--web--lvol1
    filesystem:
      fstype: ext4
      dev: /dev/HA-Web-VG/ha-web-lvol1
    tags: storage

  # (First Node) Mount /ha/ha-web
  - name: (First Node) Mount /ha/ha-web
    mount:       
      path: /ha/ha-web
      src: /dev/HA-Web-VG/ha-web-lvol1
      fstype: ext4
      state: mounted
    tags: storage

  # (First Node) restricts the list of volumes available during system boot 
  - name: (First Node) restricts the list of volumes available during system boot
    lineinfile:
      path: /etc/lvm/lvm.conf
      insertafter: '# volume_list'
      line: 'volume_list = [ "VolGroup", "@{{ ansible_hostname }}-ci", "HA-Web-VG/ha-web-lvol1" ]'

  # (First Node) Update the initial ramdisk image so the block device modules are pre-loaded during boot
  - name: (First Node) Update the initial ramdisk image so the block device modules are pre-loaded during boot
    command: dracut --hostonly --force /boot/initramfs-$(uname -r).img $(uname -r)
    tags: security

  # (First Node) Restart host after ramdisk LVM changes
  - name: (First Node) Restart host after ramdisk LVM changes
    command: shutdown -r now "Activating ramdisk LVM changes"
    tags: reboot

  # (First Node) waiting 10 mn for '{{ ansible_host }}' on port '{{ ansible_port }}' to come back
  - name: (First Node) waiting 10 mn for '{{ ansible_host }}' on port '{{ ansible_port }}' to come back
    wait_for:
      port: '{{ ansible_port }}'
      host: '{{ ansible_host }}'
      search_regex: OpenSSH
      delay: 300
      timeout: 1200 # can take longer after FS are automatically relabeled when changing SELinux from disabled mode to permissive or enforcing mode.
    become: no
  
    vars:
      ansible_connection: local
    tags: reboot

########################################
####### Others Cluster Node Hosts ######
########################################

- hosts: ha-web2 ha-web3
  become: yes
  remote_user: root

  tasks:

  # (Other Nodes) Set SE Linux to be permissive for now (will be restrictive later on)
  - name: (Other Nodes) Set SE Linux to be permissive for now (will be restrictive later on)
    lineinfile:
      path: /etc/selinux/config
      regexp: '^SELINUX='
      line: 'SELINUX=permissive'

    tags: security

  # (Other Nodes) Install necessary Packages
  - name: (Other Nodes) Install necessary Packages
    yum: name={{item}} state=installed
    with_items:
      - policycoreutils-python
      - rsync
      - lsof
      - bind-utils
      - yum-utils
      - iscsi-initiator-utils
      - device-mapper-multipath
      - "@High Availability"
      - "@Web Server"

    tags: packages

  # (Other Nodes) Enable Local Repo for Centos Updates
  - name: (Other Nodes) Enable Local Repo for Centos Updates
    yum_repository:
      name: CentOS-Updates-local
      description: CentOS-6 - Updates from local repo
      baseurl: https://192.168.1.199/shares/CENTOS%2520UPDATES/6/updates/x86_64/
      gpgkey: file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6
      repo_gpgcheck: no
      sslverify: no

    tags: updates

  # (Other Nodes) Run yum-complete-transaction to clear out any issue before next step
  - name: (Other Nodes) Run yum-complete-transaction to clear out any issue before next step
    command: /usr/sbin/yum-complete-transaction
    tags: packages


  # (Other Nodes) Update the node to take in the latest patches and security updates
  - name: (Other Nodes) Update each node to take in the latest patches and security updates
    yum:
      name: '*'
      enablerepo: CentOS-Updates-local
      state: latest

    tags: updates

  # (Other Nodes) restricts the list of volumes available during system boot
  - name: (Other Nodes) restricts the list of volumes available during system boot
    lineinfile:
      path: /etc/lvm/lvm.conf
      insertafter: '# volume_list'
      line: 'volume_list = [ "VolGroup", "@{{ ansible_hostname }}-ci", "HA-Web-VG/ha-web-lvol1" ]'

  # (Other Nodes) Restart host after SE Linux config change and latest patches and security updates
  - name: (Other Nodes) Restart host after SE Linux config change
    command: shutdown -r now "Activating ramdisk LVM changes"

    tags: reboot

  # (Other Nodes) Waiting for  host '{{ ansible_host }}' to become available
  - name: (Other Nodes) waiting 10 mn for '{{ ansible_host }}' on port '{{ ansible_port }}' to come back
    wait_for:
      port: '{{ ansible_port }}'
      host: '{{ ansible_host }}'
      search_regex: OpenSSH
      delay: 300
      timeout: 1200 # can take longer after FS are automatically relabeled when changing SELinux from disabled mode to permissive or enforcing mode.
    become: no
  
    vars:
      ansible_connection: local
    tags: reboot

  # (Other Nodes) copy files/hosts to remote hosts
  - name: (Other Nodes) copy files/hosts to remote hosts
    copy:
      src: hosts
      dest: /etc/hosts
      owner: root
      group: root
      mode: 0644
    tags: network

  # (Other Nodes) Create ifcfg-eth1
  - name: (Other Nodes) Create ifcfg-eth1
    template:
      src: ifcfg-eth1.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth1
    when: (ansible_bond0 is undefined) # run: ansible ha-web1 -i inventory -m setup -a 'filter(xxx)' to see if it's define
    ignore_errors: yes
    tags: network      

  # (Other Nodes) Create ifcfg-eth2
  - name: (Other Nodes) Create ifcfg-eth2
    template:
      src: ifcfg-eth2.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth2
    tags: network      

  # (Other Nodes) Create ifcfg-eth3
  - name: (Other Nodes) Create ifcfg-eth3
    template:
      src: ifcfg-eth3.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth3
    ignore_errors: yes
    tags: network      

   # (Other Nodes) Create ifcfg-eth4
  - name: (Other Nodes) Create ifcfg-eth4
    template:
      src: ifcfg-eth4.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth4
    tags: network      

  # (Other Nodes) Restart the network services to activate the changes
  - name: (Other Nodes) Restart the network services to activate the changes
    service:
      name: network
      state: restarted
      args: "{{ item }}"
    with_items:
      - eth1
      - eth2
      - eth3
      - eth4
    tags: network
    when: (ansible_bond0 is not defined and ansible_bond1 is not defined)

  # (Other Nodes) Create ifcfg-bondX for the public and cluster interconnect networks
  - name: (Other Nodes) Create bond configuration files for the public and cluster interconnect networks
    blockinfile:
         path: /etc/modprobe.d/bonding.conf 
         create: yes
         block: |
          alias bond0 bonding
          alias bond1 bonding
    tags: network      


  # (Other Nodes) Create ifcfg-bond0
  - name: (Other Nodes) Create ifcfg-bond0
    template:
      src: ifcfg-bond0.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-bond0
    when: (ansible_bond0 is not defined)
    ignore_errors: yes
    tags: network      

  # (Other Nodes) Create ifcfg-bond1
  - name: (Other Nodes) Create ifcfg-bond1
    template:
      src: ifcfg-bond1.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-bond1
    when: (ansible_bond1 is not defined)
    ignore_errors: yes
    tags: network

  # (Other Nodes) Modify the previous setup of ifcfg-ethX to refers the bond0
  - name: (Other Nodes) Modify the previous setup of ifcfg-ethX to refers the bonding
    template:
      src: ifcfg-eth1-bond.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth1
    when: (ansible_bond0 is defined)
    tags: network

  # (Other Nodes) Modify the previous setup of ifcfg-ethX to refers the bond1
  - name: (Other Nodes) Modify the previous setup of ifcfg-ethX to refers the bonding
    template:
      src: ifcfg-eth3-bond.j2
      dest: /etc/sysconfig/network-scripts/ifcfg-eth3
    when: (ansible_bond1 is defined)
    tags: network

  # (Other Nodes) Restart the network services to activate the changes
  - name: (Other Nodes) Restart the network services to activate the changes
    service:
      name: network
      state: restarted
      args: "{{ item }}"
    with_items:
      - eth1
      - eth2
      - eth3
      - eth4
    tags: network
    when: (ansible_bond0 is defined and ansible_bond1 is defined)

  # (Other Nodes) Configure Firewall for webservice clusters
  - name: (Other Nodes) Configure Firewall for webservice clusters
    template:
      src: templates/haweb-iptables.j2
      dest: /etc/sysconfig/iptables
    tags: network

  # (Other Nodes) Reload Firewall for webservice clusters
  - name: (Other Nodes) Reload Firewall for webservice clusters
    service:
      name: iptables
      enabled: yes
      state: restarted
    tags: network

  # (Other Nodes) start iSCSI service and enable it
  - name: (Other Nodes) start iSCSI service and enable it
    service:
      name: iscsi
      state: started
      enabled: yes
    tags: storage
      
  # (Other Nodes) Install Multipathing
  - name: (Other Nodes) Install Multipathing
    yum:
      name: device-mapper-multipath.x86_64
      state: latest
    tags: storage

  # (Other Nodes) iSCSI Discovering Targets
  - name: (Other Nodes) iSCSI Discovering Targets
    command: iscsiadm -m discovery -t st -p 192.168.1.199:3260
    tags: storage

  # (Other Nodes) iSCSI log into an individual target
  - name: (Other Nodes) iSCSI log into an individual target
    command: iscsiadm -m node -l -T iqn.1992-04.com.emc:storage.ix2-200-THUBCE.iscsi0 -p 192.168.1.199:3260
    tags: storage
 
  # (Other Nodes) Push /etc/multipath.conf from First Node to Others
  - name: (Other Nodes) Push /etc/multipath.conf from First Node to Others
    copy:
      src: files/multipath.conf
      dest: /etc/multipath.conf
    become: yes
    tags: storage

  # (Other Nodes) Start Multipathing daemon
  - name: (Other Nodes) Start Multipathing daemon
    service:
      name: multipathd
      state: started
      enabled: yes
    tags: storage

  # (Other Nodes) Ensure PV does exist
  - name: (Other Nodes) Ensure PV does exist
    stat:
      path: /dev/mapper/ha-webp1
    register: pv
    tags: storage

  - debug:
      msg: "/dev/mapper/ha-webp1 PV does exist and is a link"
    when: pv.stat.islnk is defined
    tags: storage

  # (Other Nodes) Activate lv ha-web-lvol1
  - name: (Other Nodes) Activate lv ha-web-lvol1
    lvol:
      vg: HA-Web-VG
      lv: ha-web-lvol1
      active: true
    when: pv.stat.islnk is defined
    tags: storage
    
  # (Other Nodes) update /etc/fstab
  - name: (Other Nodes) update /etc/fstab
    lineinfile:
      path: /etc/fstab
      line: '/dev/HA-Web-VG/ha-web-lvol1 /ha/ha-web ext4 defaults 0 0'
    tags: storage

  # (Other Nodes) Update the initial ramdisk image so the block device modules are pre-loaded during boot
  - name: (Other Nodes) Update the initial ramdisk image so the block device modules are pre-loaded during boot
    command: dracut --hostonly --force /boot/initramfs-$(uname -r).img $(uname -r)
    tags: security

  # (Other Nodes) Restart host after ramdisk LVM changes
  - name: (Other Nodes) Restart host after ramdisk LVM changes
    command: shutdown -r now "Activating ramdisk LVM changes"
    tags: reboot

  # (Other Nodes) Waiting for  host '{{ ansible_host }}' to become available
  - name: (Other Nodes) waiting 10 mn for '{{ ansible_host }}' on port '{{ ansible_port }}' to come back
    wait_for:
      port: '{{ ansible_port }}'
      host: '{{ ansible_host }}'
      search_regex: OpenSSH
      delay: 300
      timeout: 900
    become: no
  
    vars:
      ansible_connection: local
    tags: reboot

#####################################
####### All Cluster Node Hosts ######
#####################################

- hosts: ha-web1 ha-web2 ha-web3 ha-web-mgmt
  become: yes
  remote_user: root

  vars_prompt:

  - name: "ricci_password"
    prompt: "Enter ricci user's password:"
    private: yes
    confirm: yes
    salt_size: 8

  tasks:

  - name: (All Nodes) Create Ricci user
    user:
      name: ricci
      uid: 140

  # (All Nodes) Sync ricci's password to all nodes including Management Cluster
  - name: (All Nodes) Sync ricci's password to all nodes including Management Cluster
    lineinfile:
      path: /etc/shadow
      regexp: '^ricci'
      line: 'ricci:$1$SnwzvLa4$TzZpyEksmeNpXyZjJs3rP.:17459::::::'
    tags: clusterservice
    become: yes
 
  # (All Nodes) Set SE Linux to be persistent across reboots
  - name: (All Nodes) Set SE Linux with enforcing (was permissive so far)
    lineinfile:
      path: /etc/selinux/config
      regexp: '^SELINUX='
      line: 'SELINUX=enforcing'
    tags: security

  # (All Nodes) Install necessary Packages
  - name: (All Nodes) Install necessary Packages
    yum: 
      name: "{{ item }}"
      state: present
    with_items:
      - policycoreutils-python
      - rsync
      - lsof
      - bind-utils
      - yum-utils
      - iscsi-initiator-utils
      - device-mapper-multipath
      - "@High Availability"
      - "@Web Server"

    tags: packages

  # (All Nodes) Enable Local Repo for Centos Updates
  - name: (All Nodes) add local repo for CentOS updates
    yum_repository:
      name: CentOS-Updates-local
      description: CentOS-6 - Updates from local repo
      baseurl: https://192.168.1.199/shares/CENTOS%2520UPDATES/6/updates/x86_64/
      gpgkey: file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6
      repo_gpgcheck: no
      sslverify: no
    tags: updates

  # (All Nodes) Run yum-complete-transaction to clear out any issue before next step
  - name: (All Nodes) Run yum-complete-transaction to clear out any issue before next step
    command: /usr/sbin/yum-complete-transaction
    tags: packages


    # (All Nodes) Update the node to take in the latest patches and security updates
  - name: (All Nodes) Update each node to take in the latest patches and security updates
    yum:
      name: '*'
      enablerepo: CentOS-Updates-local
      state: latest

    tags: updates

  # (All Nodes) Restart host after SE Linux config change and latest patches and security updates
  - name: (All Nodes) Restart host after SE Linux config change
    command: shutdown -r now "Activating ramdisk LVM changes"

    tags: reboot

  # (All Nodes) Waiting for  host '{{ ansible_host }}' to become available
  - name: (All Nodes) waiting 10 mn for '{{ ansible_host }}' on port '{{ ansible_port }}' to come back
    wait_for:
      port: '{{ ansible_port }}'
      host: '{{ ansible_host }}'
      search_regex: OpenSSH
      delay: 300
      timeout: 1200
    become: no
  
    vars:
      ansible_connection: local
    tags: reboot

  # (All Nodes) Update /etc/hosts
  - name: (All Nodes) Update /etc/hosts
    copy:
      src: hosts
      dest: /etc/hosts
      owner: root
      group: root
      mode: 0644
    tags: network

  # (All Nodes) Confirm that httpd is disabled during system boot on each cluster node
  - name: (All Nodes) Confirm that httpd is disabled during system boot on each cluster node
    service:
      name: httpd
      state: stopped
      enabled: no
    tags: webservice

  # (All Nodes) Change Listen and DocumentRoot 
  - name: (All Nodes) Change Listen and DocumentRoot 
    vars:
      - Listen: "192.168.36.150:80" 
      - DocumentRoot: "/ha/ha-web"

    template:
      src: httpd.conf.j2
      dest: /etc/httpd/conf/httpd.conf
    tags: webservice
    
  # (All Nodes) Add the file context for type httpd_sys_content to the directory /ha and all contents within it
  - name: (All Nodes) Add the file context for type httpd_sys_content to the directory /ha and all contents within it
    command: /usr/sbin/semanage fcontext -a -t httpd_sys_content_t "/ha(/.*)?"

    run_once: true
    delegate_to: ha-web1
    tags: security

  # (All Nodes) Run the restorecon command to apply the changes
  - name: (All Nodes) Run the restorecon command to apply the changes
    command: /sbin/restorecon -R -v /ha

    run_once: true
    delegate_to: ha-web1
    tags: security

  # (All Nodes) Start Service ricci
  - name: (All Nodes) Start Service ricci
    service:
      name: ricci
      state: started
      enabled: yes
    delegate_to: "{{ item }}"
    with_items:
      - ha-web1
      - ha-web2
      - ha-web3

    tags: clusterservice

  # (Management Node) Start Service luci
  - name: (Management Node) Start Service luci
    service:
      name: luci
      state: started
      enabled: yes

    run_once: true
    delegate_to: ha-web-mgmt
    tags: clusterservice

  # (Management Node) Create a cluster name ha-web-cluster
  - name: (Management Node) Create a cluster name ha-web-cluster
    command: /usr/sbin/ccs --host ha-web1 --createcluster ha-web-cluster -p {{ ricci_password }} -i

    run_once: true
    delegate_to: ha-web-mgmt
    tags: clusterservice

  #  (Management Node) Add Nodes
  - name: (Management Node) Add Nodes
    command: /usr/sbin/ccs --host {{ item.host }} --addnode {{ item.node }} –-nodeid={{ item.nodeid }}
    with_items:
      - { host: 'ha-web1', node: 'ha-web1-ci', nodeid: '1' }
      - { host: 'ha-web1', node: 'ha-web2-ci', nodeid: '2' }
      - { host: 'ha-web1', node: 'ha-web3-ci', nodeid: '3' }

    run_once: true
    delegate_to: ha-web-mgmt
    tags: clusterservice

  # (Management Node) Add a fence method for the Primary fencing device
  - name: (Management Node) Add a fence method for the Primary fencing device
    command: /usr/sbin/ccs --host {{ item.host }} --addmethod Primary {{ item.fencing_method }}
    with_items:
      - { host: 'ha-web1', fencing_method: 'ha-web1-ci' }
      - { host: 'ha-web1', fencing_method: 'ha-web2-ci' }
      - { host: 'ha-web1', fencing_method: 'ha-web3-ci' }

    run_once: true
    delegate_to: ha-web-mgmt
    tags: clusterservice

  # (Management Node) Add a fence device for the IPMI LAN device
  - name: (Management Node) Add a fence device for the IPMI LAN device
    command: /usr/sbin/ccs --host {{ item.host }} --addfencedev IPMI-{{ item.fencing_device }} agent=fence_ipmilan auth=password ipaddr={{ item.ipaddr }} lanplus=on login=root name=IPMI-{{ item.fencing_device }} passwd=password power_wait=5 timeout=20
    with_items:
      - { host: 'ha-web1', fencing_device: 'ha-web1-ci', ipaddr: '192.168.36.231' }
      - { host: 'ha-web1', fencing_device: 'ha-web2-ci', ipaddr: '192.168.36.232' }
      - { host: 'ha-web1', fencing_device: 'ha-web3-ci', ipaddr: '192.168.36.233' }

    run_once: true
    delegate_to: ha-web-mgmt
    tags: clusterservice

  # (Management Node) Add a fence instance for each node to the Primary fence method
  - name: (Management Node) Add a fence instance for each node to the Primary fence method
    command: /usr/sbin/ccs --host {{ item.host }} --addfenceinst IPMI-{{ item.fencing_device }} {{ item.fencing_device }} Primary
    with_items:
      - { host: 'ha-web1', fencing_device: 'ha-web1-ci' }
      - { host: 'ha-web1', fencing_device: 'ha-web2-ci' }
      - { host: 'ha-web1', fencing_device: 'ha-web3-ci' }

    run_once: true
    delegate_to: ha-web-mgmt
    tags: clusterservice

  # (Management Node) Add Failover Domain
  - name: (Management Node) Add Failover Domain
    command: /usr/sbin/ccs --host {{ item.host }} --addfailoverdomain ha-web-failover ordered
    with_items:
      - { host: 'ha-web1' }

    run_once: true
    delegate_to: ha-web-mgmt
    tags: clusterservice

  # (Management Node) Add Failover Domain ordering 
  - name: (Management Node) Add Failover Domain ordering 
    command: /usr/sbin/ccs --host {{ item.host }} --addfailoverdomainnode ha-web-failover {{ item.fencing_device }} {{ item.order }}
    with_items:
      - { host: 'ha-web1', fencing_device: 'ha-web1-ci', order: '1' }
      - { host: 'ha-web1', fencing_device: 'ha-web2-ci', order: '2' }
      - { host: 'ha-web1', fencing_device: 'ha-web3-ci', order: '3' }

    run_once: true
    delegate_to: ha-web-mgmt
    tags: clusterservice

  # (Management Node) Add the IP Address resource (192.168.36.150)to the cluster configuration
  - name: (Management Node) Add the IP Address resource (192.168.36.150)to the cluster configuration
    command: /usr/sbin/ccs --host ha-web1 --addresource ip address=192.168.36.150 monitor_link=on sleeptime=10
    
    run_once: true
    delegate_to: ha-web-mgmt
    tags: clusterservice

  # (Management Node) Add the HA LVM resource (ha-web-HA-LVM) to the cluster configuration
  - name: (Management Node) Add the HA LVM resource (ha-web-HA-LVM) to the cluster configuration
    command: /usr/sbin/ccs --host ha-web1 --addresource lvm lv_name=ha-web-lvol1 name=ha-web-HA-LVM self_fence=on vg_name=HA-Web-VG
    
    run_once: true
    delegate_to: ha-web-mgmt
    tags: clusterservice

  # (Management Node) Add the Filesystem resource (ha-web-filesystem) to the cluster configuration
  - name: (Management Node) Add the Filesystem resource (ha-web-filesystem) to the cluster configuration
    command: /usr/sbin/ccs --host ha-web1 --addresource fs device=/dev/HA-Web-VG/ha-web-lvol1 fsid=56432 fstype=ext4 mountpoint=/ha/ha-web name=ha-web-filesystem self_fence=on
    
    run_once: true
    delegate_to: ha-web-mgmt
    tags: clusterservice

  # (Management Node) Add the Script resource (ha-web-apache-script) to the cluster configuration
  - name: (Management Node) Add the Script resource (ha-web-apache-script) to the cluster configuration
    command: /usr/sbin/ccs --host ha-web1 --addresource script file=/etc/init.d/httpd name=ha-web-apache-script
    
    run_once: true
    delegate_to: ha-web-mgmt
    tags: clusterservice

  # (Management Node) Create a service group (ha-web-service) for the webservice and add the required resources (IP, HA-LVM, Filesystem, Script) to it
  - name: (Management Node) Create a service group (ha-web-service) for the webservice and add the required resources (IP, HA-LVM, Filesystem, Script) to it
    command: /usr/sbin/ccs --host {{ item.host }} --addservice {{ item.service }} domain={{ item.domain }} max_restarts=3 name={{ item.service }} recovery=restart restart_expire_time=3600
    with_items:
      - { host: 'ha-web1', service: 'ha-web-service', domain: 'ha-web-failover' }

    run_once: true
    delegate_to: ha-web-mgmt
    tags: clusterservice

  # (Management Node) Add the required resources (IP, HA-LVM, Filesystem, Script) to service group ha-web-service
  - name: (Management Node) Add the required resources (IP, HA-LVM, Filesystem, Script) to service group ha-web-service
    command: /usr/sbin/ccs --host {{ item.host }} --addsubservice {{ item.service_group }} {{ item.sub_service }}
    with_items:
      - { host: 'ha-web1', service_group: 'ha-web-service', sub_service: 'ip ref=192.168.36.150' }
      - { host: 'ha-web1', service_group: 'ha-web-service', sub_service: 'lvm ref=ha-web-HA-LVM' }
      - { host: 'ha-web1', service_group: 'ha-web-service', sub_service: 'fs ref=ha-web-filesystem' }
      - { host: 'ha-web1', service_group: 'ha-web-service', sub_service: 'script ref=ha-web-apache-script' }
    
    run_once: true
    delegate_to: ha-web-mgmt
    tags: clusterservice

  # (Management Node) Once the cluster has been created, the configuration needs to be activated and the cluster started on all nodes
  - name: (Management Node) Once the cluster has been created, the configuration needs to be activated and the cluster started on all nodes
    command: /usr/sbin/ccs --host {{ item.host }} {{ item.command }} -p {{ ricci_password }}
    with_items:
      - { host: 'ha-web1', command: '--sync --activate' }
      - { host: 'ha-web1', command: '--checkconf' }
    
    run_once: true
    tags: clusterservice

  # (Management Node) Start the cluster services on all nodes
  - name: (Management Node) Start the cluster services on all nodes
    command: /usr/sbin/ccs --host {{ item }} --startall
    with_items:
      - ha-web1
    
    run_once: true
    tags: clusterservice
